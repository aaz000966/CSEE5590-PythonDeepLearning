{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u70o2knoVDtt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "embed_dim = 128\n",
    "lstm_out = 196 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVtPfKcXVKfy"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Negative': 8493, 'Neutral': 3142, 'Positive': 2236}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Sentiment.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = data['sentiment'].value_counts().to_dict()\n",
    "\n",
    "height = [counts['Negative'], counts['Neutral'],counts['Positive']]\n",
    "bars = ('Negative', 'Neutral', 'Positive')\n",
    "y_pos = np.arange(len(bars))\n",
    " \n",
    "plt.bar(y_pos, height)\n",
    " \n",
    "plt.xticks(y_pos, bars)\n",
    " \n",
    "plt.show()\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZjgo8n7VQA1"
   },
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvzWGl1rVRZY"
   },
   "outputs": [],
   "source": [
    "for idx, row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcn-Si3-VTMZ"
   },
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8b9O4zfVU0f"
   },
   "outputs": [],
   "source": [
    "def createmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKOVUeyJVWcZ"
   },
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
    "y = to_categorical(integer_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gyUU0LtVYIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\alfar\\Anaconda3\\envs\\cs5590\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "4578/4578 [==============================] - 1s 304us/step\n",
      "0.914157294039645\n",
      "0.659676730632782\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# model = createmodel()\n",
    "model = load_model('TwitterModel.h5')\n",
    "# model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size)\n",
    "score,acc = model.evaluate(X_test,Y_test,verbose=1,batch_size=batch_size)\n",
    "print(score)\n",
    "print(acc)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('TwitterModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('TwitterModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\n",
      "negative : 0.31 \n",
      "neutral : 0.03 \n",
      "positive : 0.67 \n"
     ]
    }
   ],
   "source": [
    "toPredict = {0: \"negative\",1: \"neutral\",2: \"positive\"}\n",
    "text = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump\"\n",
    "\n",
    "print(text)\n",
    "text = text.lower()\n",
    "text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "\n",
    "x = tokenizer.texts_to_sequences([text])\n",
    "x = pad_sequences(x, maxlen=28)\n",
    "\n",
    "x = x[[0], :]\n",
    "prediction = model.predict(x)[0]\n",
    "for key, value in toPredict.items():\n",
    "    print(value, \": %.2f \"% (prediction[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 16s 2ms/step - loss: 0.8273 - accuracy: 0.6465\n",
      "1859/1859 [==============================] - 1s 626us/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 16s 2ms/step - loss: 0.8290 - accuracy: 0.6461\n",
      "1859/1859 [==============================] - 1s 682us/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 16s 2ms/step - loss: 0.8310 - accuracy: 0.6388\n",
      "1859/1859 [==============================] - 1s 750us/step\n",
      "Epoch 1/1\n",
      "7435/7435 [==============================] - 17s 2ms/step - loss: 0.8271 - accuracy: 0.6441\n",
      "1858/1858 [==============================] - 1s 785us/step\n",
      "Epoch 1/1\n",
      "7435/7435 [==============================] - 17s 2ms/step - loss: 0.8289 - accuracy: 0.6440\n",
      "1858/1858 [==============================] - 2s 895us/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.8326 - accuracy: 0.6443\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 19s 3ms/step - loss: 0.6909 - accuracy: 0.7131\n",
      "1859/1859 [==============================] - 2s 909us/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.8235 - accuracy: 0.6494\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.6869 - accuracy: 0.7065\n",
      "1859/1859 [==============================] - 2s 962us/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.8316 - accuracy: 0.6406\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 21s 3ms/step - loss: 0.6805 - accuracy: 0.7178 \n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.8291 - accuracy: 0.6445\n",
      "Epoch 2/2\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.6810 - accuracy: 0.7124\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7435/7435 [==============================] - 23s 3ms/step - loss: 0.8207 - accuracy: 0.6432\n",
      "Epoch 2/2\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.6778 - accuracy: 0.7089\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.8302 - accuracy: 0.6421\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 24s 3ms/step - loss: 0.6915 - accuracy: 0.7043\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 24s 3ms/step - loss: 0.6134 - accuracy: 0.7432\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 24s 3ms/step - loss: 0.8263 - accuracy: 0.6447\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.6861 - accuracy: 0.7100\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.6117 - accuracy: 0.7489\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.8321 - accuracy: 0.6472\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.6863 - accuracy: 0.7074\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 21s 3ms/step - loss: 0.5980 - accuracy: 0.7568\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.8349 - accuracy: 0.6429 0s\n",
      "Epoch 2/3\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.6840 - accuracy: 0.7116\n",
      "Epoch 3/3\n",
      "7435/7435 [==============================] - 23s 3ms/step - loss: 0.5982 - accuracy: 0.7494\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7435/7435 [==============================] - 26s 4ms/step - loss: 0.8282 - accuracy: 0.6417\n",
      "Epoch 2/3\n",
      "7435/7435 [==============================] - 24s 3ms/step - loss: 0.6772 - accuracy: 0.7099\n",
      "Epoch 3/3\n",
      "7435/7435 [==============================] - 26s 4ms/step - loss: 0.5979 - accuracy: 0.7572\n",
      "1858/1858 [==============================] - 3s 1ms/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 16s 2ms/step - loss: 0.8426 - accuracy: 0.6412\n",
      "1859/1859 [==============================] - 2s 840us/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 16s 2ms/step - loss: 0.8316 - accuracy: 0.6439\n",
      "1859/1859 [==============================] - 2s 869us/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 19s 3ms/step - loss: 0.8376 - accuracy: 0.6369\n",
      "1859/1859 [==============================] - 4s 2ms/step\n",
      "Epoch 1/1\n",
      "7435/7435 [==============================] - 17s 2ms/step - loss: 0.8426 - accuracy: 0.6395\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/1\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.8324 - accuracy: 0.6409\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 19s 3ms/step - loss: 0.8385 - accuracy: 0.6396\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 19s 3ms/step - loss: 0.6869 - accuracy: 0.7041\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.8329 - accuracy: 0.6400\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 19s 3ms/step - loss: 0.6842 - accuracy: 0.7105\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.8476 - accuracy: 0.6367\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.6882 - accuracy: 0.7093\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7435/7435 [==============================] - 20s 3ms/step - loss: 0.8359 - accuracy: 0.6394\n",
      "Epoch 2/2\n",
      "7435/7435 [==============================] - 20s 3ms/step - loss: 0.6846 - accuracy: 0.7106\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/2\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.8318 - accuracy: 0.6414\n",
      "Epoch 2/2\n",
      "7435/7435 [==============================] - 21s 3ms/step - loss: 0.6768 - accuracy: 0.7163\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 21s 3ms/step - loss: 0.8396 - accuracy: 0.6414\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 20s 3ms/step - loss: 0.6941 - accuracy: 0.7015\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.6113 - accuracy: 0.7448\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.8379 - accuracy: 0.6431\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.6867 - accuracy: 0.7096\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 24s 3ms/step - loss: 0.6126 - accuracy: 0.7482\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.8392 - accuracy: 0.6353\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.6859 - accuracy: 0.7070\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.6080 - accuracy: 0.7533\n",
      "1859/1859 [==============================] - 3s 1ms/step\n",
      "Epoch 1/3\n",
      "7435/7435 [==============================] - 25s 3ms/step - loss: 0.8411 - accuracy: 0.6381\n",
      "Epoch 2/3\n",
      "7435/7435 [==============================] - 24s 3ms/step - loss: 0.6828 - accuracy: 0.7083\n",
      "Epoch 3/3\n",
      "7435/7435 [==============================] - 24s 3ms/step - loss: 0.6088 - accuracy: 0.7401\n",
      "1858/1858 [==============================] - 3s 1ms/step\n",
      "Epoch 1/3\n",
      "7435/7435 [==============================] - 26s 3ms/step - loss: 0.8325 - accuracy: 0.6369\n",
      "Epoch 2/3\n",
      "7435/7435 [==============================] - 25s 3ms/step - loss: 0.6843 - accuracy: 0.7115\n",
      "Epoch 3/3\n",
      "7435/7435 [==============================] - 25s 3ms/step - loss: 0.6054 - accuracy: 0.7443\n",
      "1858/1858 [==============================] - 4s 2ms/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.8511 - accuracy: 0.6379\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/1\n",
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.8437 - accuracy: 0.6381\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7434/7434 [==============================] - 22s 3ms/step - loss: 0.8575 - accuracy: 0.6313\n",
      "1859/1859 [==============================] - 2s 1ms/step\n",
      "Epoch 1/1\n",
      "7435/7435 [==============================] - 22s 3ms/step - loss: 0.8517 - accuracy: 0.6343\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "Epoch 1/1\n",
      "7435/7435 [==============================] - 24s 3ms/step - loss: 0.8481 - accuracy: 0.6347\n",
      "1858/1858 [==============================] - 3s 1ms/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 24s 3ms/step - loss: 0.8493 - accuracy: 0.6329\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 23s 3ms/step - loss: 0.6963 - accuracy: 0.7046\n",
      "1859/1859 [==============================] - 3s 1ms/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 25s 3ms/step - loss: 0.8451 - accuracy: 0.6356\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 24s 3ms/step - loss: 0.7012 - accuracy: 0.7022\n",
      "1859/1859 [==============================] - 3s 2ms/step\n",
      "Epoch 1/2\n",
      "7434/7434 [==============================] - 27s 4ms/step - loss: 0.8559 - accuracy: 0.6312\n",
      "Epoch 2/2\n",
      "7434/7434 [==============================] - 25s 3ms/step - loss: 0.7072 - accuracy: 0.7016\n",
      "1859/1859 [==============================] - 3s 2ms/step\n",
      "Epoch 1/2\n",
      "7435/7435 [==============================] - 26s 4ms/step - loss: 0.8529 - accuracy: 0.6327\n",
      "Epoch 2/2\n",
      "7435/7435 [==============================] - 25s 3ms/step - loss: 0.6953 - accuracy: 0.6980\n",
      "1858/1858 [==============================] - 3s 2ms/step\n",
      "Epoch 1/2\n",
      "7435/7435 [==============================] - 27s 4ms/step - loss: 0.8408 - accuracy: 0.6342\n",
      "Epoch 2/2\n",
      "7435/7435 [==============================] - 25s 3ms/step - loss: 0.6796 - accuracy: 0.7091\n",
      "1858/1858 [==============================] - 3s 2ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 30s 4ms/step - loss: 0.8452 - accuracy: 0.6322\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 29s 4ms/step - loss: 0.6943 - accuracy: 0.7058\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 28s 4ms/step - loss: 0.6232 - accuracy: 0.7359\n",
      "1859/1859 [==============================] - 3s 2ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 29s 4ms/step - loss: 0.8476 - accuracy: 0.6344\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 27s 4ms/step - loss: 0.7054 - accuracy: 0.7030\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 32s 4ms/step - loss: 0.6259 - accuracy: 0.7361\n",
      "1859/1859 [==============================] - 3s 2ms/step\n",
      "Epoch 1/3\n",
      "7434/7434 [==============================] - 33s 4ms/step - loss: 0.8458 - accuracy: 0.6298\n",
      "Epoch 2/3\n",
      "7434/7434 [==============================] - 31s 4ms/step - loss: 0.6910 - accuracy: 0.7092 1s - loss: 0\n",
      "Epoch 3/3\n",
      "7434/7434 [==============================] - 29s 4ms/step - loss: 0.6131 - accuracy: 0.7396\n",
      "1859/1859 [==============================] - 3s 2ms/step\n",
      "Epoch 1/3\n",
      "7435/7435 [==============================] - 31s 4ms/step - loss: 0.8513 - accuracy: 0.6308\n",
      "Epoch 2/3\n",
      "7435/7435 [==============================] - 30s 4ms/step - loss: 0.6878 - accuracy: 0.7072\n",
      "Epoch 3/3\n",
      "7435/7435 [==============================] - 29s 4ms/step - loss: 0.6111 - accuracy: 0.7373\n",
      "1858/1858 [==============================] - 3s 2ms/step\n",
      "Epoch 1/3\n",
      "7435/7435 [==============================] - 32s 4ms/step - loss: 0.8552 - accuracy: 0.6307\n",
      "Epoch 2/3\n",
      "7435/7435 [==============================] - 35s 5ms/step - loss: 0.6993 - accuracy: 0.7009\n",
      "Epoch 3/3\n",
      "7435/7435 [==============================] - 33s 4ms/step - loss: 0.6166 - accuracy: 0.7424 1s - loss: 0.6\n",
      "1858/1858 [==============================] - 4s 2ms/step\n",
      "Epoch 1/2\n",
      "9293/9293 [==============================] - 43s 5ms/step - loss: 0.8371 - accuracy: 0.6422\n",
      "Epoch 2/2\n",
      "9293/9293 [==============================] - 44s 5ms/step - loss: 0.6854 - accuracy: 0.7089\n",
      "Best: 0.681588 using {'batch_size': 40, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = load_model('TwitterModel.h5')\n",
    "model = KerasClassifier(build_fn=createmodel,verbose=1)\n",
    "batch_size = [10, 20, 40]\n",
    "epochs = [1, 2, 3]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['v1','v2']]\n",
    "\n",
    "data['text'] = data['v2']\n",
    "data['classification'] = data['v1']\n",
    "data = data[['text','classification']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "def createmodel():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelencoder = LabelEncoder()\n",
    "integer_encoded = labelencoder.fit_transform(data['classification'])\n",
    "y = to_categorical(integer_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 152, 128)          256000    \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "3733/3733 [==============================] - 77s 21ms/step - loss: 0.1748 - accuracy: 0.9362\n",
      "Epoch 2/7\n",
      "3733/3733 [==============================] - 74s 20ms/step - loss: 0.0414 - accuracy: 0.9874\n",
      "Epoch 3/7\n",
      "3733/3733 [==============================] - 74s 20ms/step - loss: 0.0201 - accuracy: 0.9938\n",
      "Epoch 4/7\n",
      "3733/3733 [==============================] - 77s 21ms/step - loss: 0.0122 - accuracy: 0.9968\n",
      "Epoch 5/7\n",
      "3733/3733 [==============================] - 77s 21ms/step - loss: 0.0120 - accuracy: 0.9976\n",
      "Epoch 6/7\n",
      "3733/3733 [==============================] - 76s 20ms/step - loss: 0.0053 - accuracy: 0.9981\n",
      "Epoch 7/7\n",
      "3733/3733 [==============================] - 80s 22ms/step - loss: 0.0083 - accuracy: 0.9971\n",
      "0.10184500075506007\n",
      "0.9798803925514221\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model = createmodel()\n",
    "\n",
    "model = load_model('Spam.h5')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 1)\n",
    "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
    "\n",
    "print(score)\n",
    "print(acc)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"Spam.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call now! and get free stuff!\n",
      "ham :  0.07988643\n",
      "spam :  0.92011356\n"
     ]
    }
   ],
   "source": [
    "txt = \"Call now! and get free stuff!\"\n",
    "\n",
    "print(txt)\n",
    "txt = txt.lower()\n",
    "txt = re.sub('[^a-zA-z0-9\\s]', '', txt)\n",
    "\n",
    "x = tokenizer.texts_to_sequences([txt])\n",
    "x = pad_sequences(x, maxlen=152)\n",
    "\n",
    "x = x[[0], :]\n",
    "\n",
    "prediction = model.predict(x)[0]\n",
    "toWord = {\n",
    "    0: \"ham\",\n",
    "    1: \"spam\"\n",
    "}\n",
    "for key, value in toWord.items():\n",
    "    print(value, \": \", prediction[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentimentAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
